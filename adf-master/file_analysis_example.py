"""
æ–‡ä»¶åˆ†æç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•é€šè¿‡æ–‡ä»¶è·¯å¾„ç›´æ¥åˆ†ææ•°æ®ã€‚
"""

import requests
import json
import time


def analyze_file_via_api():
    """é€šè¿‡APIåˆ†ææ–‡ä»¶"""
    
    # æœåŠ¡å™¨URL
    base_url = "http://localhost:2230"
    
    print("=" * 60)
    print("é€šè¿‡æ–‡ä»¶è·¯å¾„åˆ†æOpenSSHæ—¥å¿—æ•°æ®")
    print("=" * 60)
    
    # 1. åˆ†æOpenSSHæ—¥å¿—æ•°æ®
    print("\n1. åˆ†æOpenSSHæ—¥å¿—æ•°æ®")
    print("-" * 30)
    
    analysis_request = {
        "file_path": r"E:\software\MCP__Proj\100MCP\adf-master\OpenSSH_2k.log_structured.csv",
        "file_type": "csv",
        "analysis_type": "log_analysis",
        "time_window": "1min",
        "aggregation_method": "count",
        "regression": "c",
        "max_lags": 10,
        "lags_method": "aic",
        "save_model": True,
        "model_name": "openssh_log_analysis"
    }
    
    print(f"å‘é€åˆ†æè¯·æ±‚...")
    print(f"æ–‡ä»¶è·¯å¾„: {analysis_request['file_path']}")
    print(f"åˆ†æç±»å‹: {analysis_request['analysis_type']}")
    
    try:
        response = requests.post(f"{base_url}/tools/adf_analyze_file", json=analysis_request)
        
        if response.status_code == 200:
            result = response.json()
            print(f"è¯·æ±‚æˆåŠŸ!")
            print(f"ä»»åŠ¡çŠ¶æ€: {result['status']}")
            print(f"ä»»åŠ¡ID: {result['task_id']}")
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            task_id = result['task_id']
            print(f"\nç­‰å¾…ä»»åŠ¡å®Œæˆ...")
            
            while True:
                time.sleep(2)
                task_response = requests.post(f"{base_url}/tools/get_task", json={"task_id": task_id})
                
                if task_response.status_code == 200:
                    task_info = task_response.json()
                    progress = task_info.get('progress', 0)
                    status = task_info.get('status', 'unknown')
                    
                    print(f"è¿›åº¦: {progress:.1%} - çŠ¶æ€: {status}")
                    
                    if status == 'succeeded':
                        print("\nâœ… åˆ†æå®Œæˆ!")
                        print_result(task_info['result'])
                        break
                    elif status == 'failed':
                        print(f"\nâŒ åˆ†æå¤±è´¥: {task_info.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        break
                else:
                    print(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_response.status_code}")
                    break
        else:
            print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            
    except Exception as e:
        print(f"è¯·æ±‚å¼‚å¸¸: {e}")


def print_result(result):
    """æ‰“å°åˆ†æç»“æœ"""
    print("\n" + "=" * 60)
    print("åˆ†æç»“æœ")
    print("=" * 60)
    
    if result.get('status') == 'success':
        print(f"âœ… åˆ†ææˆåŠŸ!")
        print(f"æ–‡ä»¶è·¯å¾„: {result.get('file_path', 'N/A')}")
        print(f"åˆ†æç±»å‹: {result.get('analysis_type', 'N/A')}")
        
        # æ•°æ®æ‘˜è¦
        data_summary = result.get('data_summary', {})
        print(f"\nğŸ“Š æ•°æ®æ‘˜è¦:")
        print(f"  æ—¶é—´åºåˆ—é•¿åº¦: {data_summary.get('time_series_length', 'N/A')}")
        print(f"  æ—¶é—´èŒƒå›´: {data_summary.get('time_range', {}).get('start', 'N/A')} åˆ° {data_summary.get('time_range', {}).get('end', 'N/A')}")
        print(f"  æ•°å€¼èŒƒå›´: {data_summary.get('value_range', {}).get('min', 'N/A')} åˆ° {data_summary.get('value_range', {}).get('max', 'N/A')}")
        print(f"  å¹³å‡å€¼: {data_summary.get('value_range', {}).get('mean', 'N/A'):.2f}")
        print(f"  æ ‡å‡†å·®: {data_summary.get('value_range', {}).get('std', 'N/A'):.2f}")
        
        # ADFæ£€éªŒç»“æœ
        adf_result = result.get('adf_result', {})
        print(f"\nğŸ” ADFæ£€éªŒç»“æœ:")
        print(f"  ç»Ÿè®¡é‡: {adf_result.get('statistic', 'N/A'):.6f}")
        print(f"  på€¼: {adf_result.get('p_value', 'N/A'):.6f}")
        print(f"  æ˜¯å¦å¹³ç¨³: {'æ˜¯' if adf_result.get('is_stationary', False) else 'å¦'}")
        print(f"  æ»åé˜¶æ•°: {adf_result.get('lags_used', 'N/A')}")
        print(f"  å›å½’ç±»å‹: {adf_result.get('regression_description', 'N/A')}")
        print(f"  æ»åæ–¹æ³•: {adf_result.get('lags_method_description', 'N/A')}")
        
        # ç»“æœè§£é‡Š
        interpretation = result.get('interpretation', '')
        if interpretation:
            print(f"\nğŸ“ ç»“æœè§£é‡Š:")
            print(interpretation)
        
        # å»ºè®®
        recommendations = result.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ åˆ†æå»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        
        # æ¨¡å‹ä¿¡æ¯
        model_path = result.get('model_path')
        if model_path:
            print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    else:
        print(f"âŒ åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")


def demonstrate_natural_language_usage():
    """æ¼”ç¤ºè‡ªç„¶è¯­è¨€ä½¿ç”¨æ–¹å¼"""
    print("\n" + "=" * 60)
    print("è‡ªç„¶è¯­è¨€ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    examples = [
        {
            "natural_language": "è¯·å¸®æˆ‘åˆ†æOpenSSHæ—¥å¿—æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªADFæ¨¡å‹æ¥æ£€æµ‹æ—¥å¿—æ´»åŠ¨çš„æ—¶é—´åºåˆ—å¹³ç¨³æ€§",
            "api_call": {
                "tool": "adf_analyze_file",
                "parameters": {
                    "file_path": r"E:\software\MCP__Proj\100MCP\adf-master\OpenSSH_2k.log_structured.csv",
                    "analysis_type": "log_analysis",
                    "save_model": True,
                    "model_name": "openssh_analysis"
                }
            }
        },
        {
            "natural_language": "åˆ†æè¿™ä¸ªCSVæ–‡ä»¶ä¸­çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œçœ‹çœ‹æ˜¯å¦å¹³ç¨³",
            "api_call": {
                "tool": "adf_analyze_file",
                "parameters": {
                    "file_path": "your_data.csv",
                    "file_type": "csv",
                    "timestamp_col": "timestamp",
                    "value_col": "value",
                    "analysis_type": "full"
                }
            }
        },
        {
            "natural_language": "å¿«é€Ÿåˆ†æè¿™ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œä¸éœ€è¦ä¿å­˜æ¨¡å‹",
            "api_call": {
                "tool": "adf_analyze_file",
                "parameters": {
                    "file_path": "log_file.txt",
                    "file_type": "txt",
                    "analysis_type": "quick",
                    "save_model": False
                }
            }
        }
    ]
    
    for i, example in enumerate(examples, 1):
        print(f"\n{i}. è‡ªç„¶è¯­è¨€è¯·æ±‚:")
        print(f"   \"{example['natural_language']}\"")
        print(f"\n   å¯¹åº”çš„APIè°ƒç”¨:")
        print(f"   {json.dumps(example['api_call'], indent=2, ensure_ascii=False)}")


if __name__ == "__main__":
    print("ADFæ–‡ä»¶åˆ†æç¤ºä¾‹")
    
    # æ¼”ç¤ºè‡ªç„¶è¯­è¨€ä½¿ç”¨æ–¹å¼
    demonstrate_natural_language_usage()
    
    # å®é™…åˆ†ææ–‡ä»¶ï¼ˆéœ€è¦å…ˆå¯åŠ¨æœåŠ¡å™¨ï¼‰
    print(f"\næ³¨æ„: è¦è¿è¡Œå®é™…åˆ†æï¼Œè¯·å…ˆå¯åŠ¨MCPæœåŠ¡å™¨:")
    print(f"python adf_mcp_server.py")
    print(f"\nç„¶åå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥è¿è¡Œå®é™…åˆ†æ:")
    print(f"# analyze_file_via_api()")
    
    # å–æ¶ˆæ³¨é‡Šæ¥è¿è¡Œå®é™…åˆ†æ
    # analyze_file_via_api()
